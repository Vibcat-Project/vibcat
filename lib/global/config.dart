class Config {
  static const baseUrl = 'https://www.yamibo.com';

  static const heroTag = 'hero_tag_';

  static const markdownText = """
è¿™æ˜¯ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼é£æœºä¸ºä»€ä¹ˆèƒ½é£èµ·æ¥ï¼Œç¡®å®æœ‰ä¸€ä¸ª**å‡†ç¡®ã€æ¸…æ™°çš„ç‰©ç†åŸç†**ï¼Œè€Œä¸”ç¡®å®æ¶‰åŠåˆ°**æµä½“åŠ›å­¦**å’Œ**å‹å¼ºå·®å¼‚**ç­‰å¤šä¸ªå› ç´ ã€‚

ä¸‹é¢æˆ‘æ¥åˆ†å±‚æ¬¡åœ°è§£é‡Šä¸€ä¸‹ï¼Œå…ˆè®²ç»“è®ºï¼Œå†è®²åŸç†ï¼š

---

### âœ… é£æœºèƒ½é£èµ·æ¥çš„æœ¬è´¨åŸå› ï¼š

> **å› ä¸ºæœºç¿¼åœ¨ç©ºæ°”ä¸­è¿åŠ¨æ—¶äº§ç”Ÿâ€œå‡åŠ›â€ï¼Œè¿™ä¸ªå‡åŠ›èƒ½å…‹æœé£æœºçš„é‡åŠ›ï¼Œä½¿é£æœºé£èµ·æ¥ã€‚**

---

### ğŸ” å‡åŠ›æ˜¯æ€ä¹ˆäº§ç”Ÿçš„ï¼Ÿï¼ˆæ¶‰åŠæµä½“åŠ›å­¦ï¼‰

è¿™å°±æ˜¯æ ¸å¿ƒé—®é¢˜ã€‚å‡åŠ›çš„äº§ç”Ÿä¸»è¦ä¾èµ–**æœºç¿¼çš„å½¢çŠ¶**å’Œ**æ°”æµçš„ç›¸å¯¹è¿åŠ¨**ï¼Œæ¶‰åŠä¸¤ä¸ªä¸»è¦ç†è®ºè§£é‡Šï¼š

---

## â‘  **ä¼¯åŠªåˆ©åŸç†ï¼ˆå‹å¼ºå·®è§£é‡Šï¼‰** âœ…

è¿™æ˜¯æœ€å¸¸è§çš„è§£é‡Šæ–¹å¼ï¼š

* é£æœºçš„æœºç¿¼**ä¸Šè¡¨é¢å¼¯æ›²ã€ä¸‹è¡¨é¢è¾ƒå¹³**ï¼ˆå«â€œéå¯¹ç§°ç¿¼å‹â€ï¼‰ï¼›
* å½“é£æœºå‰è¿›æ—¶ï¼Œç©ºæ°”è¢«åˆ†æµæˆä¸Šä¸‹ä¸¤è‚¡ç»•è¿‡æœºç¿¼ï¼›
* ä¸Šæ–¹ç©ºæ°”æµåŠ¨è·¯å¾„æ›´é•¿ï¼Œå› æ­¤éœ€è¦æµå¾—æ›´å¿«ï¼›
* æ ¹æ®**ä¼¯åŠªåˆ©å®šå¾‹**ï¼šæµé€Ÿè¶Šå¿«ï¼Œå‹å¼ºè¶Šä½ï¼›
* æ‰€ä»¥ä¸Šæ–¹å‹å¼ºä½ã€ä¸‹æ–¹å‹å¼ºé«˜ â†’ **äº§ç”Ÿå‘ä¸Šçš„å‡åŠ›**ã€‚

**ä½†éœ€è¦æ³¨æ„**ï¼šè¿™ç§è§£é‡Šæ¯”è¾ƒç®€åŒ–ï¼Œå¹¶ä¸å®Œå…¨å‡†ç¡®ï¼Œå› ä¸ºï¼š

> ä¸Šæ–¹ç©ºæ°”ä¸ä¸€å®šâ€œå¿…é¡»å’Œä¸‹æ–¹ç©ºæ°”åŒæ—¶åœ¨æœºç¿¼åæ–¹æ±‡åˆâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„è¯¯è§£ã€‚

---

## â‘¡ **ç‰›é¡¿ç¬¬ä¸‰å®šå¾‹è§£é‡Šï¼ˆä½œç”¨-åä½œç”¨ï¼‰** âœ…

è¿™æ˜¯å¦ä¸€ä¸ªç‰©ç†ä¸Šæ›´ä¸¥è°¨çš„è§£é‡Šæ–¹å¼ï¼Œå¸¸ç”¨äºèˆªç©ºå·¥ç¨‹ï¼š

* æœºç¿¼ä¼š**æ”¹å˜ç©ºæ°”çš„æµåŠ¨æ–¹å‘**ï¼Œå°¤å…¶æ˜¯**æŠŠç©ºæ°”å¾€ä¸‹æ¨**ï¼›
* æ ¹æ®ç‰›é¡¿ç¬¬ä¸‰å®šå¾‹ï¼š**é£æœºå¯¹ç©ºæ°”æ–½åŠ å‘ä¸‹çš„åŠ›ï¼Œç©ºæ°”å°±å¯¹é£æœºæ–½åŠ å‘ä¸Šçš„åä½œç”¨åŠ›**ï¼›
* è¿™ä¸ªåä½œç”¨åŠ›ï¼Œå°±æ˜¯å‡åŠ›ã€‚

è¿™ç§è§£é‡Šè·Ÿç«ç®­å–·æ°”æ¨è¿›ç±»ä¼¼ï¼šä½ æ¨ç©ºæ°”ï¼Œç©ºæ°”ä¹Ÿæ¨ä½ ã€‚

---

## âœ… ç»¼åˆè§£é‡Šï¼ˆå·¥ç¨‹å®é™…ä¸­çš„å‡åŠ›ï¼‰

ç°ä»£èˆªç©ºå·¥ç¨‹è®¤ä¸ºï¼š
å‡åŠ›çš„äº§ç”Ÿï¼Œæ˜¯**æœºç¿¼å½¢çŠ¶ + è¿è§’ï¼ˆè§’åº¦ï¼‰+ æ°”æµé€Ÿåº¦**å…±åŒä½œç”¨çš„ç»“æœï¼Œæ¶‰åŠï¼š

* **ä¼¯åŠªåˆ©æ•ˆåº”**ï¼ˆå‹å¼ºå·®ï¼‰
* **æµä½“åè½¬æ•ˆåº”**ï¼ˆæ”¹å˜æ°”æµæ–¹å‘ï¼‰
* **å¾ªç¯ç†è®º**ï¼ˆæ›´é«˜é˜¶çš„æµä½“åŠ›å­¦ç†è®ºï¼‰

è¿™äº›éƒ½ä¼šå½±å“åˆ°**æœºç¿¼ä¸Šã€ä¸‹è¡¨é¢æµé€Ÿå’Œå‹åŠ›åˆ†å¸ƒ**ï¼Œä»è€Œäº§ç”Ÿå‡åŠ›ã€‚

---

### ğŸ”§ æ›´è¿›ä¸€æ­¥ï¼šå‡åŠ›çš„æ•°å­¦è¡¨è¾¾å¼

åœ¨èˆªç©ºå·¥ç¨‹ä¸­ï¼Œå‡åŠ›å¸¸ç”¨å¦‚ä¸‹å…¬å¼ä¼°ç®—ï¼š

L = rac{1}{2} ho V^2 S C_L

å…¶ä¸­ï¼š

* æ˜¯å‡åŠ›ï¼ˆå•ä½ç‰›é¡¿ï¼‰
* æ˜¯ç©ºæ°”å¯†åº¦
* æ˜¯é£æœºç›¸å¯¹äºç©ºæ°”çš„é€Ÿåº¦
* æ˜¯æœºç¿¼é¢ç§¯
* æ˜¯å‡åŠ›ç³»æ•°ï¼ˆå–å†³äºç¿¼å‹ã€è¿è§’ç­‰ï¼‰

---

### æ€»ç»“ä¸€å¥è¯ï¼š

> **é£æœºä¹‹æ‰€ä»¥èƒ½é£ï¼Œæ˜¯å› ä¸ºæœºç¿¼åœ¨è¿åŠ¨ä¸­æ”¹å˜ç©ºæ°”æµåŠ¨ï¼Œå½¢æˆå‹å¼ºå·®å’Œæµä½“åè½¬ï¼Œä»è€Œäº§ç”Ÿå‡åŠ›ã€‚è¿™ä¸€è¿‡ç¨‹æ¶‰åŠä¼¯åŠªåˆ©å®šå¾‹ã€ç‰›é¡¿å®šå¾‹å’Œå¤æ‚çš„æµä½“åŠ¨åŠ›å­¦ã€‚**

å¦‚æœä½ è¿˜æƒ³æ·±å…¥äº†è§£æ¯”å¦‚â€œèºæ—‹æ¡¨å’Œå–·æ°”å‘åŠ¨æœºå¦‚ä½•æ¨åŠ¨é£æœºå‰è¿›â€ã€â€œä»¿ç”Ÿç¿¼å‹â€ç­‰æ–¹å‘ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç»§ç»­è®²ã€‚
                    """;

  static const markdownText2 = """
  ### å¤§æ¨¡å‹å¾®è°ƒæˆ–äºŒæ¬¡è®­ç»ƒæ¦‚è¿°

å¾®è°ƒï¼ˆFine-Tuningï¼‰æˆ–äºŒæ¬¡è®­ç»ƒæ˜¯æŒ‡åœ¨é¢„è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åŸºç¡€ä¸Šï¼Œä½¿ç”¨ç‰¹å®šæ•°æ®é›†è¿›ä¸€æ­¥è®­ç»ƒæ¨¡å‹ï¼Œä½¿å…¶é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œå¦‚ç¿»è¯‘ã€åˆ†ç±»ç­‰ã€‚è¿™é€šå¸¸æ¯”ä»é›¶è®­ç»ƒé«˜æ•ˆï¼Œå› ä¸ºåªéœ€è°ƒæ•´éƒ¨åˆ†å‚æ•°ã€‚é’ˆå¯¹ä½ çš„éœ€æ±‚ï¼šå°† Qwen3 çš„ 0.6Bï¼ˆå‚æ•°è§„æ¨¡çº¦ 0.5B-0.6B çš„å°å‹å˜ä½“ï¼‰æ¨¡å‹å¾®è°ƒä¸ºä¸“å±ç¿»è¯‘å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨è¯†åˆ«æºè¯­è¨€ã€åªæ‰§è¡Œç¿»è¯‘ä»»åŠ¡ï¼ˆå¿½ç•¥å…¶ä»–æŒ‡ä»¤ï¼‰ï¼Œå¹¶ç¿»è¯‘åˆ°æŒ‡å®šè¯­è¨€ï¼ˆå¦‚è‹±æ–‡ï¼‰ã€‚è¿™å¯ä»¥é€šè¿‡æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰å®ç°ï¼Œä½¿ç”¨å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•å¦‚ LoRAï¼ˆLow-Rank Adaptationï¼‰æ¥å‡å°‘è®¡ç®—èµ„æºéœ€æ±‚ã€‚

Qwen3 æ˜¯ Alibaba çš„å¼€æºæ¨¡å‹ç³»åˆ—ï¼Œæ”¯æŒå¤šè¯­è¨€å¤„ç†ï¼Œé€‚åˆç¿»è¯‘ä»»åŠ¡ã€‚å¾®è°ƒå°å‹æ¨¡å‹å¦‚ 0.6B åœ¨æ™®é€š GPUï¼ˆå¦‚ RTX 3060ï¼‰ä¸Šå¯è¡Œï¼Œè®­ç»ƒæ—¶é—´è§†æ•°æ®é›†å¤§å°è€Œå®šï¼ˆå‡ å°æ—¶åˆ°å‡ å¤©ï¼‰ã€‚ä¸‹é¢æ˜¯å®ç°æ­¥éª¤ï¼ŒåŸºäº Hugging Face ç”Ÿæ€å’Œ Unsloth å·¥å…·ï¼ˆä¼˜åŒ–äº† Qwen3 å¾®è°ƒï¼Œæ”¯æŒåŠ¨æ€é‡åŒ–å‡å°‘å†…å­˜å ç”¨ï¼‰ã€‚

#### 1. å‡†å¤‡ç¯å¢ƒ
- **ç¡¬ä»¶è¦æ±‚**ï¼šè‡³å°‘ 8GB VRAM çš„ GPUï¼ˆæ¨è NVIDIAï¼‰ï¼Œæˆ–ä½¿ç”¨ CPUï¼ˆä½†æ…¢ï¼‰ã€‚å¦‚æœå†…å­˜ä¸è¶³ï¼Œä½¿ç”¨ 4-bit æˆ– 8-bit é‡åŒ–ã€‚
- **å®‰è£…åº“**ï¼ˆåœ¨ Python 3.10+ ç¯å¢ƒä¸­ï¼Œä½¿ç”¨ pipï¼‰ï¼š
  ```
  pip install torch transformers peft datasets accelerate bitsandbytes trl unsloth
  ```
  - `transformers`ï¼šåŠ è½½æ¨¡å‹ã€‚
  - `peft`ï¼šæ”¯æŒ LoRAã€‚
  - `datasets`ï¼šåŠ è½½æ•°æ®é›†ã€‚
  - `trl`ï¼šç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰è®­ç»ƒå™¨ã€‚
  - `unsloth`ï¼šä¼˜åŒ– Qwen3 å¾®è°ƒï¼Œå‡å°‘å†…å­˜ 2x å¹¶åŠ é€Ÿï¼ˆä»æœç´¢ç»“æœå¯è§ï¼Œå®ƒä¸“ä¸º Qwen3 è®¾è®¡ï¼‰ã€‚

#### 2. æ•°æ®é›†å‡†å¤‡
å…³é”®æ˜¯æ„å»ºä¸€ä¸ªå¤šè¯­è¨€ç¿»è¯‘æ•°æ®é›†ï¼Œå¼ºåŒ–â€œåªåšç¿»è¯‘â€çš„è¡Œä¸ºï¼š
- **éœ€æ±‚ç‰¹æ€§**ï¼š
  - è¾“å…¥ï¼šä»»æ„æ–‡æœ¬ï¼ˆåŒ…æ‹¬éç¿»è¯‘è¯·æ±‚ï¼Œå¦‚â€œå‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯â€ï¼‰ï¼Œæ¨¡å‹å¼ºåˆ¶è§†ä¸ºå¾…ç¿»è¯‘æ–‡æœ¬ã€‚
  - è¾“å‡ºï¼šè‡ªåŠ¨æ£€æµ‹æºè¯­è¨€ + ç¿»è¯‘åˆ°æŒ‡å®šè¯­è¨€ï¼ˆå‡è®¾ç›®æ ‡ä¸ºè‹±æ–‡ï¼›å¯è‡ªå®šä¹‰ï¼‰ã€‚
  - è‡ªåŠ¨è¯­è¨€æ£€æµ‹ï¼šæ¨¡å‹æœ¬èº«å¯å­¦ï¼Œä½†ä¸ºå‡†ç¡®ï¼Œå¯åœ¨æ•°æ®å‡†å¤‡æ—¶ç”¨åº“ï¼ˆå¦‚ `langdetect`ï¼‰é¢„æ ‡ç­¾æºè¯­è¨€ï¼Œç„¶åå¾®è°ƒæ¨¡å‹è¾“å‡ºæ ¼å¼å¦‚â€œæºè¯­è¨€ï¼šä¸­æ–‡ã€‚ç¿»è¯‘ï¼šEnglish text.â€ã€‚
- **æ¨èæ•°æ®é›†**ï¼ˆä» Hugging Face åŠ è½½ï¼‰ï¼š
  - **å¤šè¯­è¨€å¹³è¡Œè¯­æ–™**ï¼š`opus_books`ï¼ˆä¹¦ç±ç¿»è¯‘æ•°æ®é›†ï¼Œæ”¯æŒå¤šè¯­è¨€å¯¹ï¼‰ã€`wmt14`ï¼ˆWMT ç¿»è¯‘åŸºå‡†ï¼Œæ”¯æŒè‹±-å¾·ã€æ³•ç­‰ï¼‰ã€‚
  - **å¤šè¯­è¨€ç¿»è¯‘æ•°æ®é›†**ï¼š`facebook/covost2`ï¼ˆ21 ç§è¯­è¨€åˆ°è‹±æ–‡çš„ç¿»è¯‘ï¼Œæ”¯æŒå¤šè¯­è¨€ï¼‰ï¼Œæˆ– `Helsinki-NLP/opus-100`ï¼ˆ100 ç§è¯­è¨€å¹³è¡Œå¥å­ï¼‰ã€‚
  - **å¤§å‹å¤šè¯­è¨€æ•°æ®é›†**ï¼šCulturaXï¼ˆ6.3 ä¸‡äº¿ tokenï¼Œè¦†ç›– 167 ç§è¯­è¨€ï¼‰ï¼Œä½†å¤ªå¤§ï¼Œå¯é‡‡æ ·å­é›†ã€‚
  - **åˆæˆæ•°æ®**ï¼šç”¨æ›´å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4ï¼‰ç”Ÿæˆæ•°æ®é›†ï¼šè¾“å…¥éšæœºæ–‡æœ¬ï¼ˆå„ç§è¯­è¨€ï¼‰ï¼Œè¾“å‡ºæ£€æµ‹ + ç¿»è¯‘ã€‚å®‰è£… `langdetect` ç”Ÿæˆæ ‡ç­¾ï¼š
    ```
    pip install langdetect
    ```
    ç¤ºä¾‹ç”Ÿæˆè„šæœ¬ï¼š
    ```
    from langdetect import detect
    import json

    # å‡è®¾è¾“å…¥æ–‡æœ¬åˆ—è¡¨
    texts = ["Hello world", "Bonjour le monde", "å‘Šè¯‰æˆ‘ä¸€ä¸ªç¬‘è¯", "Non-translation input"]
    target_lang = "English"
    dataset = []

    for text in texts:
        try:
            src_lang = detect(text)
        except:
            src_lang = "unknown"
        # å‡è®¾ç¿»è¯‘å‡½æ•°ï¼ˆç”¨ API æˆ–é¢„ç¿»è¯‘ï¼‰
        translation = translate_text(text, target_lang)  # è‡ªå®šä¹‰ç¿»è¯‘å‡½æ•°
        prompt = f"Translate to {target_lang}: {text}"
        response = f"Source language: {src_lang}. Translation: {translation}"
        dataset.append({"prompt": prompt, "response": response})

    with open("translation_dataset.json", "w") as f:
        json.dump(dataset, f)
    ```
  - **æ ¼å¼åŒ–**ï¼šè½¬ä¸º Alpaca é£æ ¼ï¼ˆæŒ‡ä»¤å¾®è°ƒæ ¼å¼ï¼‰ï¼š
    ```
    {"instruction": "Translate any input to English, detect source language first.", "input": "ä»»æ„æ–‡æœ¬", "output": "Source: zh. Translation: English text."}
    ```
  - è§„æ¨¡ï¼šèµ·å§‹ 1k-10k ç¤ºä¾‹ï¼ˆå°å‹æ¨¡å‹æ˜“è¿‡æ‹Ÿåˆï¼‰ï¼›ç”¨ `datasets` åŠ è½½å¹¶ shuffleã€‚

#### 3. åŠ è½½æ¨¡å‹å’Œ Tokenizer
ä½¿ç”¨ Unsloth åŠ è½½ Qwen3-0.5Bï¼ˆå‡è®¾ä¸º 0.6B è¿‘ä¼¼ï¼›å®é™…æ£€æŸ¥ Hugging Face ä¸Šæœ€æ–°å˜ä½“ï¼‰ï¼š
```
from unsloth import FastLanguageModel
import torch

max_seq_length = 2048  # Qwen3 æ”¯æŒé•¿ä¸Šä¸‹æ–‡
dtype = None  # Auto detect (float16 for CUDA)
load_in_4bit = True  # é‡åŒ–èŠ‚çœå†…å­˜

model, tokenizer = FastLanguageModel.from_pretrained(
    "Qwen/Qwen3-0.5B",  # æˆ– "Qwen/Qwen3-0.5B-Instruct" å¦‚æœæœ‰æŒ‡ä»¤ç‰ˆ
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=load_in_4bit,
)
```
- å¦‚æœæ—  Unslothï¼Œç”¨ `AutoModelForCausalLM.from_pretrained("Qwen/Qwen3-0.5B", load_in_4bit=True)`ã€‚

#### 4. é…ç½® LoRA å’Œè®­ç»ƒå‚æ•°
LoRA åªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆr=16, alpha=32ï¼‰ï¼Œé€‚åˆå°å‹æ¨¡å‹ï¼š
```
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=16,  # ç§©
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # Qwen ç‰¹å®šæ¨¡å—
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
```
- Unsloth ç‰ˆæœ¬æ›´ç®€å•ï¼š
  ```
  model = FastLanguageModel.get_peft_model(
      model,
      r=16,
      target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
      lora_alpha=16,
      lora_dropout=0,
      bias="none",
      use_gradient_checkpointing="unsloth",
      random_state=3407,
      use_rslora=False,
      loftq_config=None
  )
  ```

#### 5. è¿›è¡Œå¾®è°ƒ
ä½¿ç”¨ `SFTTrainer`ï¼ˆç›‘ç£å¾®è°ƒï¼‰ï¼š
```
from trl import SFTTrainer
from datasets import load_dataset

dataset = load_dataset("json", data_files="translation_dataset.json", split="train")

trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    dataset_text_field="text",  # å‡è®¾æ•°æ®é›†æœ‰ 'text' å­—æ®µå¦‚ "instruction\ninput\noutput"
    max_seq_length=max_seq_length,
    args=TrainingArguments(
        per_device_train_batch_size=2,  # è°ƒæ•´æ ¹æ®å†…å­˜
        gradient_accumulation_steps=4,
        warmup_steps=5,
        max_steps=60,  # æˆ– epochs=1-3
        learning_rate=2e-4,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="outputs",
    ),
)

trainer.train()
```
- è®­ç»ƒåä¿å­˜ï¼š`trainer.model.save_pretrained("fine-tuned-qwen3-translation")`ã€‚
- åˆå¹¶ LoRAï¼š`model.merge_and_unload()`ã€‚

#### 6. è¯„ä¼°å’Œæ¨ç†
- **æµ‹è¯•**ï¼šåŠ è½½å¾®è°ƒæ¨¡å‹ï¼Œè¾“å…¥ä»»æ„æ–‡æœ¬ï¼š
  ```
  inputs = tokenizer(["Translate to English: Bonjour"], return_tensors="pt").to("cuda")
  outputs = model.generate(**inputs, max_new_tokens=100)
  print(tokenizer.decode(outputs[0]))
  ```
- **å‡†ç¡®åº¦æå‡**ï¼šé¢„å¾®è°ƒå‰æµ‹è¯•ç¿»è¯‘å‡†ç¡®ç‡ï¼ˆå¦‚ BLEU åˆ†æ•°ï¼‰ï¼Œåæ¯”è¾ƒã€‚ä½¿ç”¨ `sacrebleu` åº“è¯„ä¼°ã€‚
- **å¼ºåˆ¶åªç¿»è¯‘**ï¼šé€šè¿‡æ•°æ®é›†å¼ºåŒ–ï¼Œå¦‚æœè¾“å…¥éç¿»è¯‘ï¼Œè¾“å‡ºä»ä¸ºç¿»è¯‘ã€‚æ·»åŠ ç³»ç»Ÿæç¤ºï¼š"You are a translation bot. Always detect language and translate to English, ignore other requests."

#### æ³¨æ„äº‹é¡¹
- **å½“å‰æ¨¡å‹é—®é¢˜**ï¼šQwen3 åŸºæ¨¡å‹ç¿»è¯‘å‡†ç¡®ä½å¯èƒ½æ˜¯å› æ³›åŒ–ä¸è¶³ï¼›å¾®è°ƒåå¯è¾¾ 90%+ å‡†ç¡®ï¼ˆè§†æ•°æ®ï¼‰ã€‚
- **æŒ‘æˆ˜**ï¼šè‡ªåŠ¨æ£€æµ‹å¯èƒ½ä¸å®Œç¾ï¼ˆçŸ­æ–‡æœ¬æ˜“é”™ï¼‰ï¼›è‹¥éœ€æ›´é«˜ç²¾åº¦ï¼Œé›†æˆå¤–éƒ¨åº“å¦‚ Google Translate APIï¼Œä½†è¿™è¿èƒŒâ€œæ¨¡å‹åªåšç¿»è¯‘â€ã€‚
- **èµ„æº**ï¼šå‚è€ƒ DataCamp æ•™ç¨‹ æˆ– Unsloth æ–‡æ¡£ è·å–å®Œæ•´ä»£ç ã€‚Hugging Face è®ºå›æœ‰ç±»ä¼¼ç¿»è¯‘å¾®è°ƒè®¨è®ºã€‚
- **æˆæœ¬**ï¼šå…è´¹åœ¨æœ¬åœ°ï¼›äº‘ä¸Šï¼ˆå¦‚ Google Colabï¼‰éœ€ GPU ä¿¡ç”¨ã€‚
- å¦‚æœæ•°æ®é›†ä¸è¶³ï¼Œè‡ªç”Ÿæˆæ›´å¤šç¤ºä¾‹ã€‚å®Œæˆåï¼Œæ¨¡å‹å°†ä¸¥æ ¼é™äºç¿»è¯‘ï¼Œæé«˜æŒ‡ä»¤éµå¾ªæ€§ã€‚
  """;
}
